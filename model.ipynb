{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#load-data\" data-toc-modified-id=\"load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>load data</a></span></li><li><span><a href=\"#Viz\" data-toc-modified-id=\"Viz-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Viz</a></span></li><li><span><a href=\"#Features-selection\" data-toc-modified-id=\"Features-selection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Features selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#chi-squared-test\" data-toc-modified-id=\"chi-squared-test-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>chi-squared test</a></span></li></ul></li><li><span><a href=\"#separate-data\" data-toc-modified-id=\"separate-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>separate data</a></span></li><li><span><a href=\"#create-pipeline\" data-toc-modified-id=\"create-pipeline-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>create pipeline</a></span></li><li><span><a href=\"#Build-a-model\" data-toc-modified-id=\"Build-a-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build a model</a></span></li><li><span><a href=\"#Resultats\" data-toc-modified-id=\"Resultats-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Resultats</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "#preprocessing and model selection\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedKFold, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#regression model metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, make_scorer\n",
    "\n",
    "#regression models\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data for 2019\n",
    "path1 = \"data/caracteristiques-2019.csv\"\n",
    "path2 = \"data/lieux-2019.csv\"\n",
    "path3 = \"data/usagers-2019.csv\"\n",
    "path4 = \"data/vehicules-2019.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/caracteristiques-2019.csv does not exist: 'data/caracteristiques-2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-40d556d6a347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcaracteristiques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlieux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0musagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvehicules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File data/caracteristiques-2019.csv does not exist: 'data/caracteristiques-2019.csv'"
     ]
    }
   ],
   "source": [
    "target = 'grav'\n",
    "caracteristiques = pd.read_csv(path1, sep=';')\n",
    "lieux = pd.read_csv(path2, sep=';')\n",
    "usagers = pd.read_csv(path3, sep=';')\n",
    "vehicules = pd.read_csv(path4, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caracteristiques(caracteristiques):\n",
    "\n",
    "    # drop_caracteristiques = ['gps', 'adr', 'com'] # pour <--2018\n",
    "    drop_caracteristiques = ['adr', 'com']  # pour 2019 -->\n",
    "\n",
    "\n",
    "    new_values = \\\n",
    "['01', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
    "    '02', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n",
    "    '2A', '2B',\n",
    "    '03', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n",
    "    '04', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49',\n",
    "    '05', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59',\n",
    "    '06', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69',\n",
    "    '07', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79',\n",
    "    '08', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n",
    "    '09', '90', '91', '92', '93', '94', '95',\n",
    "    'QP', 'MQ', 'GF', 'RE', 'YT']\n",
    "\n",
    "    old_values = \\\n",
    "['10', '100', '110', '120', '130', '140', '150', '160', '170', '180', '190',\n",
    "    '20', '210', '220', '230', '240', '250', '260', '270', '280', '290',\n",
    "    '201', '202',\n",
    "    '30', '300', '310', '320', '330', '340', '350', '360', '370', '380', '390',\n",
    "    '40', '400', '410', '420', '430', '440', '450', '460', '470', '480', '490',\n",
    "    '50', '500', '510', '520', '530', '540', '550', '560', '570', '580', '590',\n",
    "    '60', '600', '610', '620', '630', '640', '650', '660', '670', '680', '690',\n",
    "    '70', '700', '710', '720', '730', '740', '750', '760', '770', '780', '790',\n",
    "    '80', '800', '810', '820', '830', '840', '850', '860', '870', '880', '890',\n",
    "    '90', '900', '910', '920', '930', '940', '950',\n",
    "    '971', '972', '973', '974', '976']\n",
    "\n",
    "    # drop columns\n",
    "    caracteristiques = caracteristiques.drop(drop_caracteristiques, axis=1)\n",
    "    # clean dep\n",
    "    caracteristiques.dep = caracteristiques.dep.replace(old_values, new_values)\n",
    "    # clean an\n",
    "    caracteristiques.loc[(caracteristiques.an < 1000),\n",
    "                         'an'] = caracteristiques.an + 2000\n",
    "    caracteristiques.an = caracteristiques.an.astype('int')\n",
    "\n",
    "    # hrmn\n",
    "    for ind in caracteristiques[(caracteristiques.hrmn.str.len() < 3)].index:\n",
    "        #print(len(caracteristiques[(caracteristiques.hrmn.str.len() < 3)].index))\n",
    "        #print(ind)\n",
    "        if caracteristiques.loc[ind, 'lum'] == 1:\n",
    "            hm = str(choice([9, 10, 11, 12, 13, 14, 15, 16, 17\n",
    "                             ])).zfill(2) + ':' + str(randint(0, 59)).zfill(2)\n",
    "        if caracteristiques.loc[ind, 'lum'] == 2:\n",
    "            hm = str(choice([6, 7, 19, 20])).zfill(2) + ':' + str(\n",
    "                randint(0, 59)).zfill(2)\n",
    "        if caracteristiques.loc[ind, 'lum'] in [3, 4, 5]:\n",
    "            hm = str(choice([23, 0, 1, 2, 3, 4])).zfill(2) + ':' + str(\n",
    "                randint(0, 59)).zfill(2)\n",
    "\n",
    "        caracteristiques.loc[ind, 'hrmn'] = hm\n",
    "\n",
    "    # separate numbers with : for time format\n",
    "    caracteristiques.hrmn = [\n",
    "        hm[:-2] + ':' + hm[-2:] if ':' not in hm else hm\n",
    "        for hm in caracteristiques.hrmn\n",
    "    ]\n",
    "\n",
    "    ## change format str to time object\n",
    "    caracteristiques.hrmn = [\n",
    "        datetime.strptime(hm, '%H:%M').time()\n",
    "        if type(hm) != type(datetime.now().time()) else hm\n",
    "        for hm in caracteristiques.hrmn\n",
    "    ]\n",
    "\n",
    "    caracteristiques['hour'] = [hm.hour for hm in caracteristiques.hrmn]\n",
    "\n",
    "    caracteristiques.drop(['hrmn'], axis=1, inplace=True)\n",
    "\n",
    "    # weekday new columns\n",
    "    caracteristiques['weekday'] = [\n",
    "        datetime(an, mois, jour).weekday() for an, mois, jour in zip(\n",
    "            caracteristiques.an, caracteristiques.mois, caracteristiques.jour)\n",
    "    ]\n",
    "\n",
    "    #lat\n",
    "    caracteristiques['lat'] = caracteristiques['lat'].str.replace(',', '.')\n",
    "    caracteristiques.lat = caracteristiques.lat.astype(float)\n",
    "    caracteristiques.loc[(caracteristiques.lat > 10000),\n",
    "                         'lat'] = caracteristiques.lat / 100000\n",
    "\n",
    "    #long\n",
    "    caracteristiques['long'] = caracteristiques['long'].str.replace(',', '.')\n",
    "    caracteristiques['long'] = [\n",
    "        np.nan if l == '-' else l for l in caracteristiques['long']\n",
    "    ]\n",
    "    caracteristiques['long'] = caracteristiques['long'].astype(float)\n",
    "    caracteristiques.loc[(caracteristiques['an'] != 2019),\n",
    "                         'long'] = caracteristiques['long'] / 100000\n",
    "\n",
    "    #atm\n",
    "    caracteristiques.atm = caracteristiques.atm.replace([np.nan, -1], 1)\n",
    "\n",
    "    return (caracteristiques)\n",
    "\n",
    "\n",
    "def clean_lieux(lieux):\n",
    "\n",
    "    # do cleaning here\n",
    "    #lieux = lieux.drop(['v1','v2','vma', 'env1'], axis = 1) # <---2018\n",
    "    lieux = lieux.drop(['v1', 'v2'], axis=1)  # 2019 -->\n",
    "\n",
    "    lieux.catr = lieux.catr.replace(np.nan, 4)\n",
    "\n",
    "    lieux.circ = lieux.circ.replace([np.nan, -1], 0)\n",
    "\n",
    "    lieux.nbv = lieux.nbv.replace([-1., np.nan], 0.)\n",
    "\n",
    "    ## Assumtion 1\n",
    "    ## Important assumtion cut all accidents where number of roads >10 or negative! Delete around 800 cases\n",
    "    lieux = lieux[(lieux.nbv <= 10) | (lieux.nbv < 0)]\n",
    "\n",
    "    lieux.vosp = lieux.vosp.replace([-1., np.nan], 0.)\n",
    "    lieux.vosp = lieux.vosp.astype(int)\n",
    "\n",
    "    lieux.prof = lieux.prof.replace([np.nan, -1.], 1.)\n",
    "\n",
    "    lieux.pr = lieux.pr.replace([np.nan, '(1)', ''], 0.)\n",
    "    lieux.pr1 = lieux.pr1.replace([np.nan, '(1)', ''], 0.)\n",
    "\n",
    "    lieux.plan = lieux.plan.replace([-1., np.nan], 0.)\n",
    "\n",
    "    lieux.surf = lieux.surf.replace([np.nan, -1., 0.])\n",
    "\n",
    "    lieux.infra = lieux.infra.replace([np.nan, -1.], 0.)\n",
    "    lieux.situ = lieux.situ.replace([np.nan, -1], 1)\n",
    "\n",
    "    ## Assumtion 2\n",
    "    #assumtion\n",
    "    lieux.lartpc = lieux.lartpc.replace(np.nan, 0.)\n",
    "    lieux.larrout = lieux.larrout.replace(np.nan, 0.)\n",
    "    \n",
    "     ## Assumtion 3\n",
    "    #assumtion cut all accidents with the speed higher that 130 km/h\n",
    "    lieux = lieux[(lieux.vma>0) & (lieux.vma<130)]\n",
    "  \n",
    "\n",
    "    return (lieux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_usagers(usagers):\n",
    "    usagers = usagers.drop(columns=\"id_vehicule\")\n",
    "    # On gère les valeurs dans les colonnes secu, pour qu'elles ressemblent à celle de la colonne secu\n",
    "\n",
    "    # <--- 2018\n",
    "    # usagers['secu'] = usagers['secu'].apply(lambda x: float(str(x)[1]) if x >= 10 else x )\n",
    "    # usagers['secu']= usagers['secu'].replace(0,usagers['secu1'].median())\n",
    "\n",
    "    usagers['secu1'] = usagers['secu1'].apply(lambda x: 3 if x == 8 else x)\n",
    "    usagers['secu1'] = usagers['secu1'].apply(lambda x: 2 if x == 0 else x)\n",
    "    usagers['secu1'] = usagers['secu1'].apply(\n",
    "        lambda x: 1 if x != 3 and x != 2 and x != -1 else x)\n",
    "    usagers['secu1'] = usagers['secu1'].replace(-1.0,\n",
    "                                                usagers['secu1'].median())\n",
    "\n",
    "    # On combine les colonnes puis on drop les colonnes secu 1,2,3\n",
    "    # usagers['secu'] = usagers['secu'].fillna(usagers['secu1'])\n",
    "    usagers = usagers.drop(columns=[\"secu1\", \"secu2\", \"secu3\"])\n",
    "\n",
    "    # Pour la colonne place, si la place n'est pas renseigné, on prend la mediane\n",
    "    usagers['place'] = usagers['place'].fillna(usagers['place'].median())\n",
    "\n",
    "    #localisation pieton locp\n",
    "\n",
    "    usagers['locp'] = usagers['locp'].replace(-1.0, np.nan)\n",
    "    usagers['locp'] = usagers['locp'].replace(9, np.nan)\n",
    "\n",
    "    # Si dans la colonne catu il s'agit d'un piéton ( valeur 3) on remplace les Nan et -1 par les même proportion\n",
    "    cond = (usagers['catu'] == 3)\n",
    "    usagers.loc[cond, 'locp'] = usagers.loc[cond, 'locp'].fillna(\n",
    "        pd.Series(\n",
    "            np.random.choice([5.0, 1.0, 4.0, 6.0, 7.0, 8.0, 3.0, 2.0],\n",
    "                             p=[\n",
    "                                 0.068641, 0.157514, 0.260471, 0.022509,\n",
    "                                 0.001271, 0.011444, 0.222107, 0.256043\n",
    "                             ],\n",
    "                             size=len(usagers))))\n",
    "\n",
    "    # de fait, les Nan restant sont forcément autres que des piétons\n",
    "\n",
    "    usagers['locp'] = usagers[\"locp\"].fillna(0)\n",
    "\n",
    "    locp = usagers[usagers.locp != 0]\n",
    "    locp.locp.value_counts(normalize=True)\n",
    "\n",
    "    # trajet, on comble les NaN pour garder les mêmes proportions\n",
    "    usagers['trajet'] = usagers['trajet'].replace(-1.0, np.nan)\n",
    "    usagers['trajet'] = usagers['trajet'].replace(0, np.nan)\n",
    "\n",
    "    usagers['trajet'].value_counts(normalize=True)\n",
    "    usagers['trajet'] = usagers['trajet'].fillna(\n",
    "        pd.Series(\n",
    "            np.random.choice([5.0, 1.0, 4.0, 9.0, 3.0, 2.0],\n",
    "                             p=[0.52, 0.18, 0.14, 0.1, 0.03, 0.03],\n",
    "                             size=len(usagers))))\n",
    "\n",
    "    #actp même méthode que pour locp\n",
    "\n",
    "    usagers['actp'] = usagers['actp'].replace([' -1', '7', '8', 'A', 'B'],\n",
    "                                              np.nan)\n",
    "\n",
    "    # Si dans la colonne catu il s'agit d'un piéton ( valeur 3) on remplace les Nan et -1 par les même proportion\n",
    "    cond = (usagers['catu'] == 3)\n",
    "    usagers.loc[cond, 'actp'] = usagers.loc[cond, 'actp'].fillna(\n",
    "        pd.Series(\n",
    "            np.random.choice([3.0, 9.0, 5.0, 1.0, 2.0, 4.0, 6.0],\n",
    "                             p=[\n",
    "                                 0.763227, 0.070539, 0.059319, 0.057122,\n",
    "                                 0.028222, 0.018598, 0.002973\n",
    "                             ],\n",
    "                             size=len(usagers))))\n",
    "\n",
    "    # de fait, les Nan restant sont forcément autres que des piétons\n",
    "\n",
    "    usagers['actp'] = usagers['actp'].fillna(0)\n",
    "    usagers['actp'] = usagers['actp'].astype(int)\n",
    "\n",
    "    #etap piéton seul ou non, peu d'info, on drop\n",
    "    usagers = usagers.drop(columns='etatp')\n",
    "\n",
    "    #année de naissance, on remplace les inconnus par la valeur médiane\n",
    "    usagers['an_nais'] = usagers['an_nais'].fillna(usagers['an_nais'].median())\n",
    "\n",
    "    return usagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vehicules(vehicules):\n",
    "    vehicules.drop(columns=['id_vehicule', 'motor','senc'], inplace = True)\n",
    "    vehicules.choc = vehicules.choc.replace(-1,0)\n",
    "    vehicules.obs = vehicules.obs.replace(-1,0)\n",
    "    vehicules.obsm = vehicules.obsm.replace(-1,0)\n",
    "    vehicules.manv = vehicules.manv.replace(-1,0) \n",
    "    return vehicules\n",
    "\n",
    "#['obs', 'obsm', 'choc', 'manv', 'vma', 'col', 'lat', 'long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristiques = clean_caracteristiques(caracteristiques)\n",
    "lieux = clean_lieux(lieux)\n",
    "vehicules = clean_vehicules(vehicules)\n",
    "usagers = clean_usagers(usagers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristiques_lieux = lieux.join(caracteristiques.set_index('Num_Acc'), on = 'Num_Acc')\n",
    "vehicules_usagers = pd.merge(usagers, vehicules, on = ['Num_Acc','num_veh'])\n",
    "df = vehicules_usagers.join(caracteristiques_lieux.set_index('Num_Acc'), on = 'Num_Acc')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['mois'], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = df_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = df.drop(['Num_Acc', 'num_veh', 'occutc', 'voie', 'long', 'lat'], axis=1)\n",
    "\n",
    "labels = [\"Indemne\", \"Tué\", \"Blessé hospitalisé\", \"Blessé léger\"]\n",
    "# Create new dataframe column with the labels instead of numbers\n",
    "df_viz[\"grav\"] = df_viz[\"grav\"].map(dict(zip(range(1, 5), labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = df_viz.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_viz.columns:\n",
    "    if df_viz[col].dtypes == 'float' :\n",
    "        df_viz[col] = df_viz[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_clean = df_info(df_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz[df_viz['catv']<40], x = 'catv', hue = 'grav', binwidth=0.99, palette = 'tab10',multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = 'catu', hue = 'grav', binwidth=0.999, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.hour, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.weekday, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.mois, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.jour, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.atm, hue = 'grav', binwidth=0.25, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.lum, hue = 'grav', binwidth=0.25, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.catu, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df_viz, x = df_viz.manv, hue = 'grav', binwidth=0.99, palette = 'tab10', multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(info_clean[info_clean['type']=='object'])\n",
    "\n",
    "\n",
    "drop_for_model = ['an', 'mois', 'jour', 'Num_Acc', 'num_veh']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['Num_Acc', 'num_veh']\n",
    "df.drop(columns=drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_null_cols = [col for col in info[info['null%']>85]['name']]\n",
    "#those are ower null columns\n",
    "delete_null_cols  #takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = df_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi-squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.an_nais.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n",
    "#separate target from features\n",
    "X = df_model[['catu', 'lum', 'place', 'manv', 'choc', 'obsm', 'vma']]#.drop(columns = [target])\n",
    "y = df_model[target]\n",
    "print(X.shape)\n",
    "\n",
    "ohc = OneHotEncoder(handle_unknown='ignore').fit(X)\n",
    "X_trans = ohc.transform(X)\n",
    "X_new = SelectPercentile(chi2, percentile=20).fit_transform(X_trans, y)\n",
    "X_new.shape\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model = df.drop(['Num_Acc', 'num_veh', 'occutc', 'voie', 'long', 'lat', 'dep', 'an', 'mois', 'jour'], axis=1)\n",
    "# df_model = df_model.dropna(axis = 0)\n",
    "\n",
    "# numerical_features = ['pr', 'pr1', 'lartpc', 'larrout', 'an_nais']\n",
    "# for col in df_model.columns:\n",
    "#     if col not in numerical_features:\n",
    "#         if df_model[col].dtypes == 'float':\n",
    "#             df_model[col] = df_model[col].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate target from features\n",
    "df_model = df\n",
    "df_model['pr'] = df_model['pr'].astype('float')\n",
    "df_model['pr1'] = df_model['pr1'].astype('float')\n",
    "\n",
    "X = df_model.drop(columns = [target])\n",
    "y = df_model[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_cols = []\n",
    "for col in df_model.columns:\n",
    "    if df_model[col].dtypes != 'object':\n",
    "        if len(df[df_model[col]<0])>0:\n",
    "            print(\"numeric: {}\".format(col))\n",
    "            negative_cols.append(col)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"object:{}\".format(col))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_model = df_info(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieton_drop = ['locp', 'actp', 'etatp']\n",
    "# pietons = df_model[df_model['catu'].isin([3,4])]\n",
    "# usagers_en_voiture = df_model[df_model['catu'].isin([1,2])]\n",
    "# usager.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.vma.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 drop\n",
    "drop_features = ['Num_Acc', 'num_veh', 'occutc', 'voie', 'long', 'lat', 'dep', 'an', 'mois', 'jour']\n",
    "\n",
    "# 2 numerical without log transformation\n",
    "numerical_features = ['pr', 'pr1', 'lartpc', 'larrout', 'an_nais', 'vma']\n",
    "\n",
    "numerical_scale_features = [\n",
    "    feature for feature in numerical_features\n",
    "    if feature not in drop_features\n",
    "]\n",
    "\n",
    "#3 categorical - dummies\n",
    "categorical_features = [feature\n",
    "    for feature in df.columns \n",
    "    if (feature not in drop_features and feature not in numerical_scale_features)\n",
    "]\n",
    "categorical_features.remove('grav')\n",
    "\n",
    "print(drop_features)\n",
    "print(categorical_features)\n",
    "print(numerical_scale_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Pipeline with transformation + model\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "numeric_scale_transformer = Pipeline(\n",
    "    steps=[('imputer1', SimpleImputer(strategy='most_frequent')), \n",
    "           ('imputer2', SimpleImputer(missing_values = -1., strategy='most_frequent')),\n",
    "           ('scaler', MinMaxScaler()) #StandardScaler() RobustScaler()), \n",
    "          ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('imputer1', SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')),\n",
    "    ('imputer2', SimpleImputer(missing_values = -1, strategy = 'most_frequent')), \n",
    "    ('imputer3', SimpleImputer(strategy = 'most_frequent')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('drop', 'drop', drop_features),\n",
    "    ('num_scal', numeric_scale_transformer, numerical_scale_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X)\n",
    "X1 = preprocessor.transform(X)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "#AdaBoostRegressor(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor',preprocessor),\n",
    "                           ('feature_selection', SelectPercentile(chi2, percentile=30)), \n",
    "                           ('classifier', AdaBoostClassifier(base_estimator = SVC(max_iter = 5000, probability = True), n_estimators=20)\n",
    ")])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformedTargetRegressor(regressor=pipeline, transformer= target_transform)\n",
    "# model\n",
    "model = pipeline\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data set on test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, #y.ravel(),\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = \\\n",
    "{'classifier__algorithm': ['SAMME.R'],\n",
    " #'classifier__base_estimator__ccp_alpha': 0.0,\n",
    " #'classifier__base_estimator__class_weight': [None],\n",
    " #'classifier__base_estimator__criterion': 'gini',\n",
    " #'classifier__base_estimator__max_depth': [8, 10, 12],\n",
    " 'classifier__base_estimator__max_features': [15, 30, 45],\n",
    " 'classifier__base_estimator': DecisionTreeClassifier(),\n",
    " 'classifier__learning_rate': [1.0, 5.],\n",
    " 'classifier__n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "\n",
    "params = \\\n",
    "{'classifier__algorithm': ['SAMME.R'],\n",
    " 'classifier__base_estimator__max_iter': [5000],\n",
    " 'classifier__learning_rate': [1.0, 5.],\n",
    " 'classifier__n_estimators': [50, 100, 150]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "cv1 = RepeatedKFold(n_splits=4, n_repeats=1, random_state=19)\n",
    "# #params\n",
    "# Grid = GridSearchCV(model, params, scoring='r2', cv=cv1, n_jobs =-1) #root_mean_square_log_error   //n_iter = 10, \n",
    "# Grid.fit(X_train, y_train) #(n,1) -> (n,)\n",
    "# best_model = Grid.best_estimator_\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, scoring = 'f1_macro', cv=cv1, error_score=\"raise\", n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the parameters of the best model\n",
    "best_model.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
